{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to clean XML converter\n",
    "This converter has 2 modes: Manual and Automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual\n",
    "The code below requires manual file input.  If you do not want this then you can comment the last line out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "### Put you file names here for the script to work\n",
    "files = [\"DVS_2014-01-01.csv\"]\n",
    "\n",
    "def clean_file(filepath):\n",
    "\n",
    "    ### Open the file, start timer and clear/declare variables\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(filepath)\n",
    "    f = open(\"clean_{}.xml\".format(filepath.split(\".\", 1)[0]), \"a\",encoding='utf-8')\n",
    "    clean_string = \"\"\n",
    "    counter = 0\n",
    "\n",
    "    ## Remove unnecesary columns and write <Root> to the xml file\n",
    "    df.drop(df.columns[0], axis=1, inplace=True) \n",
    "    df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "    f.write(\"<Root>\"+\"\\n\")\n",
    "\n",
    "\n",
    "    ## Itterate through all the rows and clean them up\n",
    "    for row_index in range(len(df)):\n",
    "\n",
    "        ### Select the text from the df\n",
    "        raw_text = df.iloc[row_index].values[0]\n",
    "        cleaned = \"\"\n",
    "\n",
    "        ## Check if certain flaws are in the text and removes them\n",
    "        if \" <?xml\" in raw_text:\n",
    "            cleaned = raw_text.split(\"\"\" <?xml\"\"\", 1)[0]\n",
    "            cleaned = cleaned.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', \"\")\n",
    "        else:\n",
    "            cleaned = raw_text.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', \"\")\n",
    "            cleaned = cleaned.split(\"\"\"    <ns0:PutReisInformatieBoodschapIn\"\"\", 1)[0]\n",
    "\n",
    "        ## Add the ezt to a string to later write to the file\n",
    "        clean_string += cleaned + \"\\n\"\n",
    "\n",
    "        ## If the script has itterated through 50 lines the data will be writen to the file. This is for preformance enhancment\n",
    "        if counter % 50 == 0:\n",
    "            f.write(clean_string)\n",
    "            clean_string = \"\"\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    ## End the file and timer\n",
    "    f.write(\"</Root>\")\n",
    "    f.close()\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Converted {} to a clean XML file in: {} seconds...\".format(filepath, end - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for file in files:\n",
    "        clean_file(file)\n",
    "        os.remove(file) #This can be commented out if you do not want to remove the files after converting them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic\n",
    "The code below searches for all the .csv files in the directory. After converting them it will automatically remove the .csv files. If you do not want this then you can comment the last line out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import glob, os\n",
    "import lzma, urllib\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "\n",
    "def clean_file(filepath):\n",
    "\n",
    "    ### Open the file, start timer and clear/declare variables\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(filepath)\n",
    "    f = open(\"clean_{}.xml\".format(filepath.split(\".\", 1)[0]), \"a\",encoding='utf-8')\n",
    "    clean_string = \"\"\n",
    "    counter = 0\n",
    "\n",
    "    ## Remove unnecesary columns and write <Root> to the xml file\n",
    "    df.drop(df.columns[0], axis=1, inplace=True) \n",
    "    df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "    f.write(\"<Root>\"+\"\\n\")\n",
    "\n",
    "\n",
    "    ## Itterate through all the rows and clean them up\n",
    "    for row_index in range(len(df)):\n",
    "\n",
    "        ### Select the text from the df\n",
    "        raw_text = df.iloc[row_index].values[0]\n",
    "        cleaned = \"\"\n",
    "\n",
    "        ## Check if certain flaws are in the text and removes them\n",
    "        if \" <?xml\" in raw_text:\n",
    "            cleaned = raw_text.split(\"\"\" <?xml\"\"\", 1)[0]\n",
    "            cleaned = cleaned.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', \"\")\n",
    "        else:\n",
    "            cleaned = raw_text.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', \"\")\n",
    "            cleaned = cleaned.split(\"\"\"    <ns0:PutReisInformatieBoodschapIn\"\"\", 1)[0]\n",
    "\n",
    "        ## Add the ezt to a string to later write to the file\n",
    "        clean_string += cleaned + \"\\n\"\n",
    "\n",
    "        ## If the script has itterated through 50 lines the data will be writen to the file. This is for preformance enhancment\n",
    "        if counter % 50 == 0:\n",
    "            f.write(clean_string)\n",
    "            clean_string = \"\"\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    ## End the file and timer\n",
    "    f.write(\"</Root>\")\n",
    "    f.close()\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Converted {} to a clean XML file in: {} seconds...\".format(filepath, end - start))\n",
    "\n",
    "def download_files(url):\n",
    "\n",
    "    ## Create a filename for the new file\n",
    "    file_name = url.split('/')[5]\n",
    "\n",
    "    ## Download the file\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "    ## Unzip the file using lzma, and write it to a .csv\n",
    "    with lzma.open(file_name, mode='rt') as file:\n",
    "        f = open(file_name.replace('.xz', ''), \"w\")\n",
    "        for line in file:\n",
    "            f.write(line)\n",
    "        f.close\n",
    "\n",
    "    ## Remove the .xz file\n",
    "    os.remove(file_name)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        download_files(url)\n",
    "    \n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        clean_file(file)\n",
    "        os.remove(file) #This can be commented out if you do not want to remove the files after converting them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLS\n",
    "Here are all the download links for every Month. You can copy the month you need and fill \"urls\" with it.\n",
    "I wont do a whole year at once bcs you would need more than 365 GB free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2013():\n",
    "    urls_2013_11 = [\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-09.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-10.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-11.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-12.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-14.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-15.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-16.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-17.csv.xz',\n",
    "        'https://trein.fwrite.org/dumps/2013-11/DVS_2013-11-18.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-19.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-20.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-21.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-22.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-23.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-24.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-25.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-26.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-27.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-28.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-29.csv.xz',\n",
    "        'https://trein.fwrite.org/AMS-RPi/2013-11/DVS_2013-11-30.csv.xz',   \n",
    "    ]\n",
    "    urls_2013_12 = []\n",
    "    for day in range(30):\n",
    "        urls_2013_12.append('https://trein.fwrite.org/AMS-RPi/2013-12/DVS_2013-12-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "def create_2014():\n",
    "\n",
    "    urls_2014_01 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_01.append('https://trein.fwrite.org/AMS-RPi/2014-01/DVS_2014-01-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_02 = []\n",
    "    for day in range(27):\n",
    "        urls_2014_02.append('https://trein.fwrite.org/AMS-RPi/2014-02/DVS_2014-02-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_03 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_03.append('https://trein.fwrite.org/AMS-RPi/2014-03/DVS_2014-03-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_04 = []\n",
    "    for day in range(29):\n",
    "        urls_2014_04.append('https://trein.fwrite.org/AMS-RPi/2014-04/DVS_2014-04-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_05 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_05.append('https://trein.fwrite.org/AMS-RPi/2014-05/DVS_2014-05-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_06 = []\n",
    "    for day in range(29):\n",
    "        urls_2014_06.append('https://trein.fwrite.org/AMS-RPi/2014-06/DVS_2014-06-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_07 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_07.append('https://trein.fwrite.org/AMS-RPi/2014-07/DVS_2014-07-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_08 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_08.append('https://trein.fwrite.org/AMS-RPi/2014-08/DVS_2014-08-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_09 = []\n",
    "    for day in range(29):\n",
    "        urls_2014_09.append('https://trein.fwrite.org/AMS-RPi/2014-09/DVS_2014-09-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_10 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_10.append('https://trein.fwrite.org/AMS-RPi/2014-10/DVS_2014-10-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_11 = []\n",
    "    for day in range(29):\n",
    "        urls_2014_11.append('https://trein.fwrite.org/AMS-RPi/2014-11/DVS_2014-11-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2014_12 = []\n",
    "    for day in range(30):\n",
    "        urls_2014_12.append('https://trein.fwrite.org/AMS-RPi/2014-12/DVS_2014-12-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "def create_2015():\n",
    "\n",
    "    urls_2015_01 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_01.append('https://trein.fwrite.org/AMS-RPi/2015-01/DVS_2015-01-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_02 = []\n",
    "    for day in range(27):\n",
    "        urls_2015_02.append('https://trein.fwrite.org/AMS-RPi/2015-02/DVS_2015-02-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_03 = []\n",
    "    for day in range(28):\n",
    "        urls_2015_03.append('https://trein.fwrite.org/DT-HP/2015-03/DVS_2015-03-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_04 = []\n",
    "    for day in range(29):\n",
    "        urls_2015_04.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-04/DVS_2015-04-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_05 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_05.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-05/DVS_2015-05-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_06 = []\n",
    "    for day in range(29):\n",
    "        urls_2015_06.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-06/DVS_2015-06-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_07 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_07.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-07/DVS_2015-07-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_08 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_08.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-08/DVS_2015-08-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_09 = []\n",
    "    for day in range(29):\n",
    "        urls_2015_09.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-09/DVS_2015-09-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_10 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_10.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-10/DVS_2015-10-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_11 = []\n",
    "    for day in range(29):\n",
    "        urls_2015_11.append('https://trein.fwrite.org/DT-RPi-archive/2015-11/DVS_2015-11-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2015_12 = []\n",
    "    for day in range(30):\n",
    "        urls_2015_12.append('https://trein.fwrite.org/AMS-Aurora-archive/2015-12/DVS_2015-12-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "def create_2016():\n",
    "\n",
    "    urls_2016_01 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_01.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-01/DVS_2016-01-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_02 = []\n",
    "    for day in range(28):\n",
    "        urls_2016_02.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-02/DVS_2016-02-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_03 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_03.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-03/DVS_2016-03-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_04 = []\n",
    "    for day in range(29):\n",
    "        urls_2016_04.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-04/DVS_2016-04-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_05 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_05.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-05/DVS_2016-05-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_06 = []\n",
    "    for day in range(29):\n",
    "        urls_2016_06.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-06/DVS_2016-06-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_07 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_07.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-07/DVS_2016-07-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_08 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_08.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-08/DVS_2016-08-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_09 = []\n",
    "    for day in range(29):\n",
    "        urls_2016_09.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-09/DVS_2016-09-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_10 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_10.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-10/DVS_2016-10-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_11 = []\n",
    "    for day in range(29):\n",
    "        urls_2016_11.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-11/DVS_2016-11-{}}.csv.xz'.format(day+1))\n",
    "\n",
    "    urls_2016_12 = []\n",
    "    for day in range(30):\n",
    "        urls_2016_12.append('https://trein.fwrite.org/AMS-Aurora-archive/2016-12/DVS_2016-12-{}}.csv.xz'.format(day+1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
